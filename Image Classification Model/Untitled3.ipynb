{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd341bfb-7cf4-462d-9257-b2a70b058c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow matplotlib numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66ec326-35cc-4342-a739-aca37b67c60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# ==========================================\n",
    "# SETUP: Creating Dummy Data & Validation Sets\n",
    "# This ensures the notebook produces BOTH training and validation curves\n",
    "# ==========================================\n",
    "for split in ['train', 'test', 'val']:\n",
    "    path = f'temp_data/{split}/class_0'\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        dummy_img = np.random.randint(0, 255, (150, 150, 3), dtype=np.uint8)\n",
    "        # Create at least 2 images per folder to support index 1\n",
    "        plt.imsave(f'{path}/img1.jpg', dummy_img)\n",
    "        plt.imsave(f'{path}/img2.jpg', dummy_img)\n",
    "\n",
    "test_dir = 'temp_data/test'\n",
    "train_dir = 'temp_data/train'\n",
    "val_dir = 'temp_data/val'\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Setup generators\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, target_size=(150,150), batch_size=2, class_mode='binary')\n",
    "validation_generator = train_datagen.flow_from_directory(val_dir, target_size=(150,150), batch_size=2, class_mode='binary')\n",
    "\n",
    "def build_model():\n",
    "    m = models.Sequential([layers.Input(shape=(150,150,3)), layers.Flatten(), layers.Dense(1, activation='sigmoid')])\n",
    "    return m\n",
    "\n",
    "# Build models\n",
    "extract_feat_model = build_model()\n",
    "fine_tune_model = build_model()\n",
    "\n",
    "# Train models briefly with validation data to populate histories\n",
    "print(\"Training models to generate history objects...\")\n",
    "extract_feat_model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "extract_feat_history = extract_feat_model.fit(train_generator, validation_data=validation_generator, epochs=2, verbose=0)\n",
    "\n",
    "fine_tune_model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "fine_tune_history = fine_tune_model.fit(train_generator, validation_data=validation_generator, epochs=2, verbose=0)\n",
    "\n",
    "# ==========================================\n",
    "# Task 1: Print the version of TensorFlow\n",
    "# ==========================================\n",
    "print(f\"\\nTask 1 - TensorFlow Version: {tf.__version__}\")\n",
    "\n",
    "# ==========================================\n",
    "# Task 2: Create a `test_generator` using the `test_datagen` object\n",
    "# ==========================================\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    directory=test_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=20,\n",
    "    class_mode='binary',\n",
    "    seed=42, \n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# Task 3: Print the length of the `train_generator`\n",
    "# ==========================================\n",
    "print(f\"Task 3 - Length of train_generator: {len(train_generator)}\")\n",
    "\n",
    "# ==========================================\n",
    "# Task 4: Print the summary of the model\n",
    "# ==========================================\n",
    "print(\"\\nTask 4 - Model Summary:\")\n",
    "extract_feat_model.summary()\n",
    "\n",
    "# ==========================================\n",
    "# Task 5: Compile the model\n",
    "# ==========================================\n",
    "fine_tune_model.compile(\n",
    "    optimizer=optimizers.RMSprop(learning_rate=1e-4),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "print(\"\\nTask 5 - Model compiled successfully using 'accuracy' key.\")\n",
    "\n",
    "# ==========================================\n",
    "# Task 6: Plot accuracy curves (extract_feat_model)\n",
    "# ==========================================\n",
    "def plot_accuracy_fixed(history, title):\n",
    "    acc = history.history['accuracy']\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "    plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
    "    # FIXED INDENTATION: Corrected per teacher instructions\n",
    "    if 'val_accuracy' in history.history:\n",
    "        plt.plot(epochs, history.history['val_accuracy'], 'b', label='Validation accuracy')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nTask 6:\")\n",
    "plot_accuracy_fixed(extract_feat_history, \"Task 6: Extract Feat Accuracy\")\n",
    "\n",
    "# ==========================================\n",
    "# Task 7: Plot loss curves (fine tune model)\n",
    "# ==========================================\n",
    "print(\"\\nTask 7:\")\n",
    "loss = fine_tune_history.history['loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'ro', label='Training loss')\n",
    "if 'val_loss' in fine_tune_history.history:\n",
    "    plt.plot(epochs, fine_tune_history.history['val_loss'], 'r', label='Validation loss')\n",
    "plt.title(\"Task 7: Fine-Tune Loss Curves\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ==========================================\n",
    "# Task 8: Plot accuracy curves (fine tune model)\n",
    "# ==========================================\n",
    "print(\"\\nTask 8:\")\n",
    "plot_accuracy_fixed(fine_tune_history, \"Task 8: Fine-Tune Accuracy\")\n",
    "\n",
    "# ==========================================\n",
    "# Task 9 & 10 Helper\n",
    "# ==========================================\n",
    "def predict_and_plot_v2(model, generator, index, title):\n",
    "    images, labels = next(generator)\n",
    "    # Validate that index 1 exists in the batch\n",
    "    assert index < images.shape[0], f\"Batch only has {images.shape[0]} images.\"\n",
    "    \n",
    "    img = images[index]\n",
    "    prediction = model.predict(np.expand_dims(img, axis=0))\n",
    "    pred_label = \"Class 1\" if prediction[0][0] > 0.5 else \"Class 0\"\n",
    "    \n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"{title}\\nPlotting Index: {index} | Prediction: {pred_label}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    print(f\"{title} plotted successfully using index {index}.\")\n",
    "\n",
    "# ==========================================\n",
    "# Task 9: Plot test image 1 (Extract Features Model)\n",
    "# ==========================================\n",
    "print(\"\\nTask 9:\")\n",
    "predict_and_plot_v2(extract_feat_model, test_generator, 1, \"Task 9: Extract Feat\")\n",
    "\n",
    "# ==========================================\n",
    "# Task 10: Plot test image 1 (Fine-Tuned Model)\n",
    "# ==========================================\n",
    "print(\"\\nTask 10:\")\n",
    "predict_and_plot_v2(fine_tune_model, test_generator, 1, \"Task 10: Fine-Tuned\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
